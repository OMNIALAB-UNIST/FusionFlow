{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fetchNprepNtrain', 'prepNtrain', 'train_only', 'prep_only']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, shutil, argparse, re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--batch', type=int, default=4)\n",
    "# parser.add_argument('--all', action='store_tru e', default=False)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# dir_pattern = './log/pure_dataloading_b{}_worker{}_epoch1_iter10/'\n",
    "logdir=\"./log/DDP\"\n",
    "\n",
    "# print(logtype)\n",
    "dir_pattern = logdir+'/{}/{}/epoch{}/b{}/worker{}/thread{}/'\n",
    "output_file=\"output.log\"\n",
    "batchsize=1024\n",
    "worker=24\n",
    "thread=0\n",
    "# model resnet18 only\n",
    "# logtypes=os.listdir(logdir)\n",
    "logtypes=['train_only','prepNtrain','fetchNprepNtrain']\n",
    "models=['resnet18']\n",
    "datasets={'train_only':['1024batch'], \n",
    "          'prepNtrain':['1024batch10'], \n",
    "          'fetchNprepNtrain':['size5','size10','size20','size160']\n",
    "         }\n",
    "epochs={'train_only':'50', \n",
    "        'prepNtrain':'50',\n",
    "        'fetchNprepNtrain':'5'\n",
    "         }\n",
    "term={'train_only':'GPU stall', \n",
    "        'prepNtrain':'Prep stall',\n",
    "        'fetchNprepNtrain':'Fetch stall'\n",
    "         }\n",
    "print(logtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prep_only'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9ab83efa5d88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mbreakdown_col_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Step\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Iteration time (sec)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Training stall time (sec)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Throughput (image/sec)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdir_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_dirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9ab83efa5d88>\u001b[0m in \u001b[0;36mget_dirname\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlogtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdir_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mdir_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mdir_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prep_only'"
     ]
    }
   ],
   "source": [
    "def get_dirname():\n",
    "    dir_names = {}\n",
    "    for logtype in logtypes:\n",
    "        dir_names[logtype]=[]\n",
    "        for dataset in datasets[logtype]:\n",
    "            dir_path = dir_pattern.format(logtype,dataset,epochs[logtype], batchsize, worker, thread)\n",
    "            dir_names[logtype].append(os.path.abspath(dir_path))\n",
    "\n",
    "    return dir_names\n",
    "\n",
    "def replace_str(target):\n",
    "    target=target.replace('\\n', '')\n",
    "    target=target.replace(',', '')\n",
    "    return target\n",
    "\n",
    "def find_value(arr, target, jumpto=1):\n",
    "    try:\n",
    "        num=replace_str(arr[arr.index(target)+jumpto])\n",
    "    except:\n",
    "        print(arr, target, arr[arr.index(target)+jumpto])\n",
    "        num='NA'\n",
    "    return num\n",
    "\n",
    "parsed_dir=\"dsanalyzer_parsed\"\n",
    "os.makedirs(parsed_dir, exist_ok=True)\n",
    "\n",
    "breakdown_col_name=[\"Epoch\",\"Step\",\"Iteration time (sec)\",\"Training stall time (sec)\",\"Throughput (image/sec)\"]\n",
    "dir_names=get_dirname()\n",
    "print(dir_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_prep(prepro_time, breakdown_parsed_log, data_idx, epoch_idx):\n",
    "    prepro_time=prepro_time.replace(\"DataPreprocessingTime:\",\"\")\n",
    "    if(data_idx<epoch_idx):\n",
    "        breakdown_parsed_log[data_idx].append(prepro_time)\n",
    "    else:\n",
    "        breakdown_parsed_log.append([prepro_time])\n",
    "    return data_idx+1\n",
    "\n",
    "def parser(dir_names, datasets):\n",
    "    for logtype in logtypes:\n",
    "        for logdir in dir_names[logtype]:\n",
    "            dataset=logdir.split('/')[-5]\n",
    "            parse_filename=logtype+\"_\"+dataset+\"_worker\"+str(worker)+\"_epoch\"+epochs[logtype]\n",
    "\n",
    "            logfile=logdir +\"/\"+ output_file\n",
    "            breakdown_parsed_log = []\n",
    "\n",
    "            data_idx=0\n",
    "            epoch_idx=0\n",
    "            for line in open(logfile, 'r').readlines():\n",
    "                if line.startswith(f'Epoch:'): # start log\n",
    "                    replace_txt=line.replace('\\t',' ')\n",
    "                    test =replace_txt.split(' ')\n",
    "                    info = list(filter(lambda x: x != \"\", test))\n",
    "#                         print(info)\n",
    "\n",
    "                    epochNstep=find_value(info, \"Epoch:\")\n",
    "                    epoch=epochNstep.split(']')[0].replace('[','')\n",
    "                    if epochNstep.find('/') != -1:\n",
    "                        step=epochNstep.split('[')[2].split('/')[0]\n",
    "                    else:\n",
    "                        step=find_value(info, \"Epoch:\",2).split('/')[0]\n",
    "\n",
    "                    iter_time=find_value(info, \"Time\") # epoch avg: total n iter\n",
    "                    data_time=find_value(info, \"Data\")# data time\n",
    "                    throughput=find_value(info, \"Throughput\")# throughput avg\n",
    "                    cur_log_info=[epoch,step,iter_time,data_time,throughput]\n",
    "#                         print(cur_log_info)\n",
    "                    breakdown_parsed_log.append(cur_log_info)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            breakdown_df = pd.DataFrame(breakdown_parsed_log,\n",
    "                              columns=breakdown_col_name)\n",
    "            breakdown_df.dropna().to_csv(parsed_dir+\"/\"+parse_filename+\".csv\", sep=',', na_rep='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser(dir_names, datasets)\n",
    "# parser_epoch0(workers, dir_names2, datasets[1])\n",
    "# parser_epoch0(workers, dir_names3, datasets[2])\n",
    "# parser(workers, dir_names4, datasets[3],0)\n",
    "\n",
    "# parser(workers, dir_names, datasets[0],1)\n",
    "# # parser(workers, dir_names2, datasets[1])\n",
    "# # parser(workers, dir_names3, datasets[2])\n",
    "# parser(workers, dir_names4, datasets[3],1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_col_name=[\"Log type\",\"Dataset\",\"Avg iteration time (sec)\", \"Avg training stall time (sec) time(sec)\", \"Avg throughput (images/sec)\"]\n",
    "\n",
    "# def get_avg(target_col):\n",
    "#     avg_throughput_log=[]\n",
    "def total_summary():\n",
    "    total_log=[]\n",
    "    for logtype in logtypes:\n",
    "            for logdir in dir_names[logtype]:\n",
    "                dataset=logdir.split('/')[-5]\n",
    "                parse_filename=logtype+\"_\"+dataset+\"_worker\"+str(worker)\n",
    "                df=pd.read_csv(f\"./{parsed_dir}/{parse_filename}_epoch{epochs[logtype]}.csv\",index_col=None)\n",
    "                df=df[df[\"Step\"] > 4]\n",
    "                df=df[df[\"Epoch\"] > 0]\n",
    "                iter_avg=df[\"Iteration time (sec)\"].mean()\n",
    "                data_avg=df[\"Training stall time (sec)\"].mean()\n",
    "                throughput_avg=df[\"Throughput (image/sec)\"].mean()\n",
    "\n",
    "                total_log.append([term[logtype],dataset,iter_avg,data_avg,throughput_avg])\n",
    "                    \n",
    "    avg_df = pd.DataFrame(total_log,\n",
    "                              columns=dataset_col_name)\n",
    "    avg_df.to_csv(parsed_dir+\"/df_analyzer_total_summary_epoch\"+epochs[logtype]+\".csv\", sep=',', na_rep='NA')\n",
    "    \n",
    "total_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_col_name=[\"Worker\",\"Thread\",\"Avg iteration time (sec)\", \"Avg data time(sec)\", \"Avg throughput (images/sec)\",\"Avg data preprocessing time (sec)\"]\n",
    "# # thread_col_name=[\"Worker\",1,2,4,8]\n",
    "# thread_col_name=[\"Worker\",0]\n",
    "# for dataset in [\"size20\",\"size160\"]:\n",
    "#     total_log=[]\n",
    "#     total_throughput_log=[]\n",
    "#     total_data_time_log=[]\n",
    "#     total_preprocessing_time_log=[]\n",
    "    \n",
    "#     for worker in workers:\n",
    "#         worker_num=int(worker.replace(\"worker\",\"\"))\n",
    "#         worker_throughput_log=[worker_num,0]\n",
    "#         worker_data_time_log=[worker_num,0]\n",
    "#         worker_preprocessing_time_log=[worker_num,0]\n",
    "        \n",
    "#         for thread in threads:\n",
    "#             thread_num=int(thread.replace(\"thread\",\"\"))\n",
    "#             df=pd.read_csv(f\"./parsed/{dataset}_{worker}_{thread}_epoch0.csv\",index_col=None)\n",
    "#             df=df[5:].dropna().astype(float)\n",
    "#             iter_avg=df[\"Iteration time (sec)\"].mean()\n",
    "#             data_avg=df[\"Data time (sec)\"].mean()\n",
    "#             throughput_avg=df[\"Throughput (image/sec)\"].mean()\n",
    "#             data_prepro_avg=df[\"Data preprocessing time (sec)\"].mean()\n",
    "            \n",
    "#             if thread_num is 4:\n",
    "#                 thread_idx=3\n",
    "#             elif thread_num is 8:\n",
    "#                 thread_idx=4\n",
    "#             else:\n",
    "#                 thread_idx=thread_num\n",
    "#             worker_throughput_log[thread_idx]=throughput_avg\n",
    "#             worker_data_time_log[thread_idx]=data_avg\n",
    "#             worker_preprocessing_time_log[thread_idx]=data_prepro_avg\n",
    "#             total_log.append([worker_num,thread_num,iter_avg,data_avg,throughput_avg,data_prepro_avg])\n",
    "# #             break\n",
    "# #         print(total_log)\n",
    "# #         break\n",
    "# #     break\n",
    "#         total_throughput_log.append(worker_throughput_log)\n",
    "#         total_data_time_log.append(worker_data_time_log)\n",
    "#         total_preprocessing_time_log.append(worker_preprocessing_time_log)\n",
    "        \n",
    "#     prepro_df = pd.DataFrame(total_preprocessing_time_log,\n",
    "#                               columns=thread_col_name)\n",
    "#     prepro_df.sort_values(by=['Worker'], axis=0, inplace=True)\n",
    "#     prepro_df.to_csv(parsed_dir+\"/\"+dataset+\"_\"+\"preprocessing_summary_epoch0.csv\", sep=',', na_rep='NA')\n",
    "    \n",
    "#     tput_df = pd.DataFrame(total_throughput_log,\n",
    "#                               columns=thread_col_name)\n",
    "#     tput_df.sort_values(by=['Worker'], axis=0, inplace=True)\n",
    "#     tput_df.to_csv(parsed_dir+\"/\"+dataset+\"_\"+\"throughput_summary_epoch0.csv\", sep=',', na_rep='NA')\n",
    "    \n",
    "#     dataTime_df = pd.DataFrame(total_data_time_log,\n",
    "#                               columns=thread_col_name)\n",
    "#     dataTime_df.sort_values(by=['Worker'], axis=0, inplace=True)\n",
    "\n",
    "#     dataTime_df.to_csv(parsed_dir+\"/\"+dataset+\"_\"+\"dataTime_summary_epoch0.csv\", sep=',', na_rep='NA')\n",
    "    \n",
    "#     avg_df = pd.DataFrame(total_log,\n",
    "#                               columns=dataset_col_name)\n",
    "#     avg_df.sort_values(by=['Worker'], axis=0, inplace=True)\n",
    "#     avg_df.to_csv(parsed_dir+\"/\"+dataset+\"_\"+\"total_summary_epoch0.csv\", sep=',', na_rep='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_col_name=[\"Worker\",\"Thread\",\"Avg iteration time (sec)\", \"Avg data time(sec)\", \"Avg throughput (images/sec)\",\"Avg data preprocessing time (sec)\"]\n",
    "# thread_col_name=[\"Worker\",1,2,4,8]\n",
    "# for dataset in datasets:\n",
    "#     total_log=[]\n",
    "#     total_throughput_log=[]\n",
    "#     total_data_time_log=[]\n",
    "#     total_preprocessing_time_log=[]\n",
    "    \n",
    "#     for worker in workers:\n",
    "#         worker_num=int(worker.replace(\"worker\",\"\"))\n",
    "#         worker_throughput_log=[worker_num,1,2,4,8]\n",
    "#         worker_data_time_log=[worker_num,1,2,4,8]\n",
    "#         worker_preprocessing_time_log=[worker_num,1,2,4,8]\n",
    "        \n",
    "#         for thread in threads:\n",
    "#             thread_num=int(thread.replace(\"thread\",\"\"))\n",
    "#             df=pd.read_csv(f\"./parsed/{dataset}_{worker}_{thread}_epoch0.csv\",index_col=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
