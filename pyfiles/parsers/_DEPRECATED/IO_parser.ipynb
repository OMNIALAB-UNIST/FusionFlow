{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil, argparse, re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import training_info\n",
    "\n",
    "logdir=training_info.logdir\n",
    "dir_pattern = logdir+'/{}/{}/{}/{}/epoch{}/b{}/worker{}/thread{}/'\n",
    "aug_pattern = logdir+'/{}/{}/'\n",
    "simple_dir_pattern = logdir+'/{}'\n",
    "logtypes=training_info.logtypes\n",
    "datasets=training_info.datasets\n",
    "models=training_info.models\n",
    "term=training_info.term\n",
    "output_file=\"ssd_io.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_only': ['./log/DDP/train_only/1024batch10/default/resnet18/epoch5/b1024/worker4/thread0/', './log/DDP/train_only/1024batch/default/resnet18/epoch5/b1024/worker2/thread0/', './log/DDP/train_only/1024batch/default/resnet18/epoch5/b1024/worker24/thread0/', './log/DDP/train_only/1024batch/default/resnet18/epoch5/b256/worker24/thread0/'], 'loadNtrain': ['./log/DDP/loadNtrain/size5/default/resnet18/epoch5/b256/worker2/thread0/', './log/DDP/loadNtrain/1024batch10/default/resnet18/epoch5/b1024/worker2/thread0/', './log/DDP/loadNtrain/1024batch10/default/resnet18/epoch5/b1024/worker24/thread0/', './log/DDP/loadNtrain/1024batch10/default/resnet18/epoch5/b256/worker24/thread0/', './log/DDP/loadNtrain/1024batch/default/resnet18/epoch5/b1024/worker2/thread0/'], 'prepNloadNtrain': ['./log/DDP/prepNloadNtrain/size5/default/resnet18/epoch5/b1024/worker4/thread0/', './log/DDP/prepNloadNtrain/size5/default/resnet18/epoch5/b1024/worker16/thread0/', './log/DDP/prepNloadNtrain/size5/default/resnet18/epoch5/b1024/worker8/thread0/', './log/DDP/prepNloadNtrain/size5/augmix/resnet18/epoch5/b1024/worker1/thread0/', './log/DDP/prepNloadNtrain/size5/augmix/resnet18/epoch5/b1024/worker2/thread0/', './log/DDP/prepNloadNtrain/size5/augmix/resnet18/epoch5/b1024/worker4/thread0/', './log/DDP/prepNloadNtrain/size5/randaugment/resnet18/epoch5/b1024/worker1/thread0/', './log/DDP/prepNloadNtrain/size5/randaugment/resnet18/epoch5/b1024/worker2/thread0/', './log/DDP/prepNloadNtrain/size5/randaugment/resnet18/epoch5/b1024/worker4/thread0/', './log/DDP/prepNloadNtrain/size2/default/resnet18/epoch5/b1024/worker4/thread0/', './log/DDP/prepNloadNtrain/size2/augmix/resnet18/epoch5/b1024/worker4/thread0/', './log/DDP/prepNloadNtrain/size2/randaugment/resnet18/epoch5/b1024/worker4/thread0/', './log/DDP/prepNloadNtrain/size3/default/resnet18/epoch5/b1024/worker2/thread0/', './log/DDP/prepNloadNtrain/size3/default/resnet18/epoch5/b1024/worker4/thread0/', './log/DDP/prepNloadNtrain/size3/default/resnet18/epoch5/b1024/worker16/thread0/', './log/DDP/prepNloadNtrain/size3/default/resnet18/epoch5/b1024/worker8/thread0/', './log/DDP/prepNloadNtrain/size3/augmix/resnet18/epoch5/b1024/worker2/thread0/', './log/DDP/prepNloadNtrain/size3/augmix/resnet18/epoch5/b1024/worker4/thread0/', './log/DDP/prepNloadNtrain/size3/randaugment/resnet18/epoch5/b1024/worker2/thread0/', './log/DDP/prepNloadNtrain/size3/randaugment/resnet18/epoch5/b1024/worker4/thread0/'], 'fsNprepNloadNtrain': ['./log/DDP/fsNprepNloadNtrain/size5/default/resnet18/epoch5/b1024/worker2/thread0/'], 'fetchNfsNprepNloadNtrain': [], 'prepNloadNtrainNoPin': []}\n"
     ]
    }
   ],
   "source": [
    "def get_dirname():\n",
    "    dir_names = {}\n",
    "    for logtype in logtypes:\n",
    "        dir_names[logtype]=glob.glob(simple_dir_pattern.format(logtype)+f\"/*/*/*/*/*/*/*/\")\n",
    "\n",
    "    return dir_names\n",
    "\n",
    "def replace_str(target):\n",
    "    target=target.replace('\\n', '')\n",
    "    target=target.replace(',', '')\n",
    "    return target\n",
    "\n",
    "def find_value(arr, target, jumpto=1):\n",
    "    try:\n",
    "        num=replace_str(arr[arr.index(target)+jumpto])\n",
    "    except:\n",
    "        print(arr, target, arr[arr.index(target)+jumpto])\n",
    "        num='NA'\n",
    "    return num\n",
    "\n",
    "parsed_dir=training_info.parsed_dir + \"/io\"\n",
    "os.makedirs(parsed_dir, exist_ok=True)\n",
    "\n",
    "breakdown_col_name=[\"TPS\",\"KB read/s\",\"KB write/s\",\"KB read\", \"KB write\"]\n",
    "dir_names=get_dirname()\n",
    "print(dir_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parser(dir_names, datasets):\n",
    "    for logtype in logtypes:\n",
    "        for logdir in dir_names[logtype]:\n",
    "            if not os.path.exists(logdir):\n",
    "                    continue\n",
    "            logdir_list=logdir.split('/')\n",
    "            \n",
    "            dataset=find_value(logdir_list, logtype)\n",
    "            aug=find_value(logdir_list, dataset)\n",
    "            model=find_value(logdir_list, aug)\n",
    "            epoch=find_value(logdir_list, model)\n",
    "            batchsize=find_value(logdir_list, epoch)\n",
    "            worker=find_value(logdir_list, batchsize)\n",
    "                             \n",
    "            parse_filename=f\"{logtype}_{dataset}_{model}_{aug}_{worker}_{epoch}_{batchsize}_io\"\n",
    "            \n",
    "            logfile=logdir +\"/\"+ output_file\n",
    "            breakdown_parsed_log = []\n",
    "\n",
    "            for line in open(logfile, 'r').readlines():\n",
    "                if line.startswith(f'sdc1'): # start log\n",
    "                    replace_txt=line.replace('\\t',' ')\n",
    "                    test =replace_txt.split(' ')\n",
    "                    info = list(filter(lambda x: x != \"\", test))\n",
    "                    tps = find_value(info,\"sdc1\")\n",
    "                    kB_readPs = find_value(info,\"sdc1\",2)\n",
    "                    kB_wrtnPs = find_value(info,\"sdc1\",3)\n",
    "                    kB_read = find_value(info,\"sdc1\",4)\n",
    "                    kB_wrtn = find_value(info,\"sdc1\",5)\n",
    "                    breakdown_parsed_log.append([tps,kB_readPs,kB_wrtnPs,kB_read,kB_wrtn])\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            breakdown_df = pd.DataFrame(breakdown_parsed_log,\n",
    "                              columns=breakdown_col_name)\n",
    "            breakdown_df.dropna().to_csv(parsed_dir+\"/\"+parse_filename+\".csv\", sep=',', na_rep='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser(dir_names, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
